# 默认 TorchRL PPO 训练配置
seed: 42
device: "cuda:0"
# 非对称观测配置（Actor使用policy观测，Critic使用critic观测）
policy_obs_key: "policy"
critic_obs_key: "critic"
action_key: "action"

collector:
  frames_per_batch: 32768
  total_frames: 10000000
  init_random_frames: 0
  max_frames_per_traj: null

policy_model:
  hidden_sizes: [256, 256]
  activation: "relu"
  scale_mapping: "biased_softplus_1.0"
  scale_lb: 1.0e-04

value_model:
  hidden_sizes: [256, 256]
  activation: "relu"

optimizer:
  lr: 0.0003
  betas: [0.9, 0.999]
  weight_decay: 0.0

ppo:
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.0
  value_loss_coef: 0.5
  normalize_advantage: true
  num_epochs: 4
  mini_batch_size: 4096
  grad_clip: 1.0
