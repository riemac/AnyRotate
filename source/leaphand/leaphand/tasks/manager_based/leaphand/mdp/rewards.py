# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""LeapHandè¿ç»­æ—‹è½¬ä»»åŠ¡çš„å¥–åŠ±å‡½æ•°"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING, Sequence

from isaaclab.assets import Articulation, RigidObject
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils.math import quat_conjugate, quat_mul, wrap_to_pi, quat_from_angle_axis
from isaaclab.markers import VisualizationMarkers
from isaaclab.markers.config import BLUE_ARROW_X_MARKER_CFG

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def rotation_velocity(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    visualize_actual_axis: bool = True,
    target_angular_speed: float = 1.5,
    positive_decay: float = 5.0,
    negative_penalty_weight: float = 2.0,
) -> torch.Tensor:
    """è®¡ç®—æ—‹è½¬é€Ÿåº¦å¥–åŠ± - ç›®æ ‡æ˜¯è¾¾åˆ°æŒ‡å®šçš„è§’é€Ÿåº¦è€Œéè¶Šå¿«è¶Šå¥½

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: ç‰©ä½“èµ„äº§é…ç½®
        visualize_actual_axis: æ˜¯å¦å¯è§†åŒ–å®é™…æ—‹è½¬è½´
        target_angular_speed: ç›®æ ‡è§’é€Ÿåº¦å¤§å° (rad/s)
        positive_decay: æ­£å‘å¥–åŠ±çš„æŒ‡æ•°è¡°å‡å› å­
        negative_penalty_weight: è´Ÿå‘æƒ©ç½šçš„æƒé‡ç³»æ•°

    Returns:
        æ—‹è½¬é€Ÿåº¦å¥–åŠ± (num_envs,)

    NOTE:
        æ—‹è½¬è½´æ˜¯ç»•çš„ä¸–ç•Œåæ ‡ç³»ä¸­çš„å›ºå®šè½´æ—‹è½¬ï¼Œè€Œä¸æ˜¯ç»•ç‰©ä½“è‡ªèº«çš„å±€éƒ¨åæ ‡ç³»è½´æ—‹è½¬
        ç‰©ä½“æ—‹è½¬æ—¶çš„æ—‹è½¬è½´å’ŒBody Frameçš„è¡¨ç¤ºæ— å…³

        å¥–åŠ±å…¬å¼ï¼š
        - æ­£å‘é€Ÿåº¦: R = exp(-positive_decay * |projected_velocity - target_angular_speed|)
        - è´Ÿå‘é€Ÿåº¦: R = negative_penalty_weight * projected_velocity (è´Ÿæƒ©ç½š)
    """
    # è·å–ç‰©ä½“èµ„äº§
    asset: RigidObject = env.scene[asset_cfg.name]

    # è·å–å½“å‰ç‰©ä½“æ—‹è½¬
    current_object_rot = asset.data.root_quat_w # å›ºå®šçš„ä¸–ç•Œåæ ‡ç³»

    # åˆå§‹åŒ–last_object_rotå¦‚æœä¸å­˜åœ¨
    if not hasattr(env, 'last_object_rot'):
        env.last_object_rot = torch.zeros((env.num_envs, 4), dtype=torch.float, device=env.device)
        env.last_object_rot[:, 0] = 1.0  # åˆå§‹åŒ–ä¸ºå•ä½å››å…ƒæ•°

    # è®¡ç®—æ—‹è½¬å·®å¼‚
    quat_diff = quat_mul(current_object_rot, quat_conjugate(env.last_object_rot))
    angle = 2.0 * torch.acos(torch.clamp(torch.abs(quat_diff[:, 0]), max=1.0))

    # è®¡ç®—æ—‹è½¬è½´
    axis = quat_diff[:, 1:4]
    axis_norm = torch.norm(axis, dim=-1, keepdim=True) # è®¡ç®—æ—‹è½¬è½´çš„èŒƒæ•°
    valid_rotation = axis_norm.squeeze(-1) > 1e-6 #  # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ—‹è½¬(èŒƒæ•°å¤§äºé˜ˆå€¼) omega^hat*thetaï¼Œè‹¥ç‰©ä½“é™æ­¢ä¸åŠ¨ï¼Œè¯¥èŒƒå¼å°†éå¸¸å°
    axis = torch.where(valid_rotation.unsqueeze(-1), axis / axis_norm, torch.zeros_like(axis)) # å¯¹æœ‰æ•ˆæ—‹è½¬è¿›è¡Œå½’ä¸€åŒ–,æ— æ•ˆæ—‹è½¬ç½®é›¶

    # è·å–ç›®æ ‡æ—‹è½¬è½´ - ä»Commandç®¡ç†å™¨è·å–
    rotation_axis = env.command_manager.get_command("rotation_axis")

    # è®¡ç®—æ²¿æŒ‡å®šæ—‹è½¬è½´çš„è§’é€Ÿåº¦
    angular_velocity = angle / env.step_dt
    projected_velocity = torch.sum(axis * rotation_axis, dim=-1) * angular_velocity

    # æ›´æ–°ä¸Šä¸€å¸§æ—‹è½¬
    env.last_object_rot[:] = current_object_rot.clone()

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨å¥–æƒ©ä¸€ä½“çš„é€»è¾‘ ---
    # 1. å¯¹äºæ­£å‘é€Ÿåº¦ (æ–¹å‘æ­£ç¡®)
    #    æˆ‘ä»¬ä½¿ç”¨æŒ‡æ•°è¡°å‡å½¢å¼ï¼Œé¼“åŠ±é€¼è¿‘ç›®æ ‡é€Ÿåº¦
    speed_error_positive = torch.abs(projected_velocity - target_angular_speed)
    # ğŸ”¥ é™åˆ¶æŒ‡æ•°å‚æ•°ï¼Œé˜²æ­¢exp()æº¢å‡º
    # exp_arg = torch.clamp(-positive_decay * speed_error_positive, min=-10.0, max=10.0)
    # reward_positive = torch.exp(exp_arg)

    # 2. å¯¹äºè´Ÿå‘é€Ÿåº¦ (æ–¹å‘é”™è¯¯)
    #    æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªçº¿æ€§çš„æƒ©ç½šé¡¹ã€‚é€Ÿåº¦è¶Šè´Ÿï¼Œæƒ©ç½šè¶Šå¤§ã€‚
    #    projected_velocityæ˜¯è´Ÿçš„ï¼Œæ‰€ä»¥ä¹˜ä»¥ä¸€ä¸ªæ­£æƒé‡å°±å˜æˆäº†è´Ÿçš„å¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
    reward_negative = negative_penalty_weight * projected_velocity

    # 3. ä½¿ç”¨ torch.where æ ¹æ®é€Ÿåº¦æ–¹å‘é€‰æ‹©å¥–åŠ±/æƒ©ç½š
    #    å½“ projected_velocity > 0 æ—¶ï¼Œé‡‡ç”¨ reward_positive
    #    å¦åˆ™ (<= 0)ï¼Œé‡‡ç”¨ reward_negative
    reward = torch.where(
        projected_velocity > 0,
        reward_positive,
        reward_negative
    )

    # ğŸ”¥ æœ€ç»ˆçš„NaN/Infæ£€æŸ¥
    # reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
    #                     torch.zeros_like(reward), reward)

    # å¯è§†åŒ–å®é™…æ—‹è½¬è½´
    if visualize_actual_axis:
        _visualize_actual_rotation_axis(env, asset, axis, valid_rotation)

    return reward


def _visualize_actual_rotation_axis(
    env: ManagerBasedRLEnv,
    asset: RigidObject,
    actual_axis: torch.Tensor,
    valid_rotation: torch.Tensor,
):
    """å¯è§†åŒ–å®é™…æ—‹è½¬è½´

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset: ç‰©ä½“èµ„äº§
        actual_axis: å®é™…æ—‹è½¬è½´ (num_envs, 3)
        valid_rotation: æœ‰æ•ˆæ—‹è½¬æ©ç  (num_envs,)
    """
    # åˆå§‹åŒ–å¯è§†åŒ–å™¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not hasattr(env, '_actual_axis_visualizer'):
        # åˆ›å»ºè“è‰²ç®­å¤´å¯è§†åŒ–å™¨
        marker_cfg = BLUE_ARROW_X_MARKER_CFG.replace(
            prim_path="/Visuals/Reward/actual_rotation_axis"
        )
        # è®¾ç½®ç®­å¤´å°ºå¯¸ï¼ˆä¸ç›®æ ‡è½´ç›¸åŒï¼‰
        marker_cfg.markers["arrow"].scale = (0.1, 0.1, 0.3)
        env._actual_axis_visualizer = VisualizationMarkers(marker_cfg)

    # åªæ˜¾ç¤ºæœ‰æ•ˆæ—‹è½¬çš„ç®­å¤´
    valid_env_ids = valid_rotation.nonzero(as_tuple=False).squeeze(-1)
    if len(valid_env_ids) == 0:
        return

    # è·å–ç‰©ä½“ä½ç½®
    object_pos_w = asset.data.root_pos_w[valid_env_ids]

    # è®¡ç®—ç®­å¤´ä½ç½®ï¼ˆç‰©ä½“ä¸Šæ–¹ï¼Œä½†ä¸ç›®æ ‡è½´æœ‰ä¸åŒåç§»é¿å…é‡å ï¼‰
    arrow_positions = object_pos_w.clone()
    arrow_positions[:, 2] += 0.20  # æ¯”ç›®æ ‡è½´ç¨é«˜ä¸€äº›ï¼ˆç›®æ ‡è½´æ˜¯0.15ï¼‰

    # ç›´æ¥ä½¿ç”¨å®é™…æ—‹è½¬è½´è®¡ç®—ç®­å¤´æ–¹å‘
    valid_axes = actual_axis[valid_env_ids]
    arrow_orientations = _compute_arrow_orientations_from_axis(valid_axes, env.device)

    # åˆ›å»ºmarker_indices
    marker_indices = torch.zeros(len(valid_env_ids), device=env.device, dtype=torch.int32)

    # æ›´æ–°å¯è§†åŒ–
    env._actual_axis_visualizer.visualize(
        translations=arrow_positions,
        orientations=arrow_orientations,
        marker_indices=marker_indices
    )


def _compute_arrow_orientations_from_axis(axis: torch.Tensor, device: torch.device) -> torch.Tensor:
    """ä»æ—‹è½¬è½´è®¡ç®—ç®­å¤´æ–¹å‘å››å…ƒæ•°

    Args:
        axis: å·²å½’ä¸€åŒ–çš„æ—‹è½¬è½´å‘é‡ (num_envs, 3)
        device: è®¾å¤‡

    Returns:
        ç®­å¤´æ–¹å‘å››å…ƒæ•° (num_envs, 4) - (w, x, y, z)

    Note:
        è¾“å…¥çš„axiså·²ç»åœ¨rotation_velocity_rewardä¸­è¢«å½’ä¸€åŒ–ï¼Œæ— éœ€é‡å¤å½’ä¸€åŒ–
    """
    # ç®­å¤´é»˜è®¤æ–¹å‘ï¼ˆXè½´æ­£æ–¹å‘ï¼‰
    default_direction = torch.tensor([1.0, 0.0, 0.0], device=device)

    # è®¡ç®—æ—‹è½¬è½´ï¼ˆå‰ç§¯ï¼‰å’Œè§’åº¦ï¼ˆç‚¹ç§¯ï¼‰
    rotation_axis = torch.cross(default_direction.unsqueeze(0).expand_as(axis), axis, dim=-1)
    cos_angle = torch.sum(default_direction.unsqueeze(0) * axis, dim=-1)

    # è®¡ç®—æ—‹è½¬è§’åº¦
    angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0))

    # å¤„ç†æ—‹è½¬è½´ä¸ºé›¶çš„æƒ…å†µï¼ˆå‘é‡å¹³è¡Œï¼‰
    rotation_axis_norm = torch.norm(rotation_axis, dim=-1, keepdim=True)
    rotation_axis = torch.where(
        rotation_axis_norm > 1e-6,
        rotation_axis / rotation_axis_norm,
        torch.tensor([0.0, 0.0, 1.0], device=device).unsqueeze(0).expand_as(rotation_axis)
    )

    # ä½¿ç”¨Isaac Labå®˜æ–¹å‡½æ•°è®¡ç®—å››å…ƒæ•°
    orientations = quat_from_angle_axis(angle, rotation_axis)

    return orientations


def fingertip_distance_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """è®¡ç®—æŒ‡å°–åˆ°ç‰©ä½“ä¸­å¿ƒè·ç¦»çš„æƒ©ç½š - é¼“åŠ±æŒ‡å°–æ¥è¿‘ç‰©ä½“

    Args:
        env: ç¯å¢ƒå®ä¾‹
        object_cfg: ç‰©ä½“èµ„äº§é…ç½®
        robot_cfg: æœºå™¨äººèµ„äº§é…ç½®

    Returns:
        æŒ‡å°–è·ç¦»æƒ©ç½š (num_envs,)
        
    Note:
        å¥–åŠ±å…¬å¼ï¼šR = mean(||fingertip_pos - object_pos||_2)
        å…¶ä¸­fingertip_posæ˜¯æ¯ä¸ªæŒ‡å°–çš„ä¸–ç•Œåæ ‡ä½ç½®ï¼Œobject_posæ˜¯ç‰©ä½“è´¨å¿ƒçš„ä¸–ç•Œåæ ‡ä½ç½®
        ä½¿ç”¨å¹³å‡è·ç¦»è€Œä¸æ˜¯æœ€å°è·ç¦»å¯ä»¥è®©æ‰€æœ‰æ‰‹æŒ‡éƒ½å‚ä¸æŠ“å–ï¼Œé¿å…éƒ¨åˆ†æ‰‹æŒ‡ä¸æ“ä½œ
    """
    # è·å–ç‰©ä½“èµ„äº§
    object_asset: RigidObject = env.scene[object_cfg.name]
    robot_asset: Articulation = env.scene[robot_cfg.name]

    # è·å–ç‰©ä½“è´¨é‡ä¸­å¿ƒä½ç½®
    object_pos_w = object_asset.data.root_pos_w

    # LeapHandæŒ‡å°–bodyåç§°ï¼ˆåŸºäºå®é™…çš„body_namesè¾“å‡ºï¼‰
    fingertip_body_names = [
        "fingertip",         # é£ŸæŒ‡æŒ‡å°–
        "thumb_fingertip",   # æ‹‡æŒ‡æŒ‡å°–
        "fingertip_2",       # ä¸­æŒ‡æŒ‡å°–
        "fingertip_3"        # æ— åæŒ‡æŒ‡å°–
    ]

    # è·å–æ‰€æœ‰æŒ‡å°–çš„ä½ç½®
    fingertip_distances = []
    
    for body_name in fingertip_body_names:
        # è·å–æŒ‡å°–bodyçš„ä¸–ç•Œåæ ‡ä½ç½®
        body_indices, _ = robot_asset.find_bodies(body_name)
        # è‹¥æ— åŒ¹é…ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        if len(body_indices) == 0:
            raise IndexError(f"Body not found: {body_name}")
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„ç´¢å¼•ï¼ˆPython intï¼‰
        body_idx = int(body_indices[0])
        fingertip_pos_w = robot_asset.data.body_pos_w[:, body_idx]
        
        # è®¡ç®—æŒ‡å°–åˆ°ç‰©ä½“ä¸­å¿ƒçš„è·ç¦»
        distance = torch.norm(fingertip_pos_w - object_pos_w, p=2, dim=-1)
        fingertip_distances.append(distance)

    # å°†æ‰€æœ‰æŒ‡å°–è·ç¦»å †å ä¸ºå¼ é‡ (num_envs, num_fingertips)
    fingertip_distances_tensor = torch.stack(fingertip_distances, dim=-1)
    # è®¡ç®—æœ€å°è·ç¦»ï¼ˆæœ€æ¥è¿‘ç‰©ä½“çš„æŒ‡å°–ï¼‰æˆ–å¹³å‡è·ç¦»ï¼ˆæ›´å¹³æ»‘ï¼‰
    # min_distance = torch.min(fingertip_distances_tensor, dim=-1)[0]
    distance = torch.mean(fingertip_distances_tensor, dim=-1)
    # è¿”å›å½¢çŠ¶ (num_envs,)
    return distance


def rotation_axis_alignment_reward(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    theta_tolerance: float = 0.1,
    decay_factor: float = 5.0,
) -> torch.Tensor:
    """è®¡ç®—æ—‹è½¬è½´å®¹å¿å·®å¥–åŠ± - æŒ‡æ•°è¡°å‡å‹å¥–åŠ±

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: ç‰©ä½“èµ„äº§é…ç½®
        theta_tolerance: è§’åº¦å®¹å¿åº¦ (å¼§åº¦)
        decay_factor: æŒ‡æ•°è¡°å‡å› å­

    Returns:
        æ—‹è½¬è½´å¯¹é½å¥–åŠ± (num_envs,)

    Note:
        å¥–åŠ±å…¬å¼ï¼šR_axis = weight * exp(-decay_factor * max(0, theta - theta_tolerance))
        å…¶ä¸­thetaæ˜¯å®é™…æ—‹è½¬è½´ä¸ç›®æ ‡æ—‹è½¬è½´ä¹‹é—´çš„å¤¹è§’
    """
    # è·å–ç‰©ä½“èµ„äº§
    asset: RigidObject = env.scene[asset_cfg.name]

    # è·å–å½“å‰ç‰©ä½“æ—‹è½¬
    current_object_rot = asset.data.root_quat_w

    # åˆå§‹åŒ–ç‹¬ç«‹çš„last_object_rotçŠ¶æ€ï¼ˆé¿å…ä¸rotation_velocity_rewardå†²çªï¼‰
    if not hasattr(env, 'last_object_rot_alignment'):
        env.last_object_rot_alignment = torch.zeros((env.num_envs, 4), dtype=torch.float, device=env.device)
        env.last_object_rot_alignment[:, 0] = 1.0  # åˆå§‹åŒ–ä¸ºå•ä½å››å…ƒæ•°

    # è®¡ç®—æ—‹è½¬å·®å¼‚
    quat_diff = quat_mul(current_object_rot, quat_conjugate(env.last_object_rot_alignment))

    # è®¡ç®—å®é™…æ—‹è½¬è½´
    axis = quat_diff[:, 1:4]
    axis_norm = torch.norm(axis, dim=-1, keepdim=True)
    valid_rotation = axis_norm.squeeze(-1) > 1e-6
    axis = torch.where(valid_rotation.unsqueeze(-1), axis / axis_norm, torch.zeros_like(axis))

    # è·å–ç›®æ ‡æ—‹è½¬è½´
    target_axis = env.command_manager.get_command("rotation_axis")

    # è®¡ç®—å®é™…æ—‹è½¬è½´ä¸ç›®æ ‡æ—‹è½¬è½´ä¹‹é—´çš„å¤¹è§’
    # ä½¿ç”¨ç‚¹ç§¯è®¡ç®—å¤¹è§’ï¼šcos(theta) = aÂ·b / (|a||b|)
    dot_product = torch.sum(axis * target_axis, dim=-1)
    # ğŸ”¥ æ›´ä¸¥æ ¼çš„æ•°å€¼ç¨³å®šæ€§å¤„ç†
    dot_product = torch.clamp(dot_product, -0.9999, 0.9999)  # é¿å…acos(Â±1)çš„æ•°å€¼é—®é¢˜

    # ğŸ”¥ å®‰å…¨çš„è§’åº¦è®¡ç®—ï¼Œå¤„ç†æ— æ•ˆæ—‹è½¬çš„æƒ…å†µ
    theta = torch.where(
        valid_rotation,
        torch.acos(torch.abs(dot_product)),  # åªè®¡ç®—ç»å¯¹å€¼çš„è§’åº¦ï¼Œé¿å…æ–¹å‘é—®é¢˜
        torch.zeros_like(dot_product)        # æ— æ•ˆæ—‹è½¬æ—¶è§’åº¦ä¸º0
    )

    # è®¡ç®—æŒ‡æ•°è¡°å‡å¥–åŠ±
    angle_error = torch.clamp(theta - theta_tolerance, min=0.0)
    reward = torch.exp(-decay_factor * angle_error)

    # ğŸ”¥ é¢å¤–çš„NaNæ£€æŸ¥å’Œå¤„ç†
    reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
                        torch.zeros_like(reward), reward)

    # æ›´æ–°ä¸Šä¸€å¸§æ—‹è½¬ï¼ˆç‹¬ç«‹çŠ¶æ€ï¼‰
    env.last_object_rot_alignment[:] = current_object_rot.clone()

    return reward


def grasp_reward(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    target_pos_offset: tuple[float, float, float] = (0.0, -0.1, 0.56)
) -> torch.Tensor:
    """è®¡ç®—æŠ“å–å¥–åŠ± - é¼“åŠ±ä¿æŒç‰©ä½“åœ¨æ‰‹ä¸­

    Args:
        env: ç¯å¢ƒå®ä¾‹
        object_cfg: ç‰©ä½“èµ„äº§é…ç½®
        target_pos_offset: ç›®æ ‡ä½ç½®åç§»ï¼ˆç¯å¢ƒå±€éƒ¨åæ ‡ç³»ï¼‰

    Returns:
        æŠ“å–å¥–åŠ± (num_envs,)
    """
    # è·å–ç‰©ä½“èµ„äº§
    object_asset: RigidObject = env.scene[object_cfg.name]

    # è·å–ç‰©ä½“ä½ç½®ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
    object_pos_w = object_asset.data.root_pos_w

    # è½¬æ¢ä¸ºç¯å¢ƒå±€éƒ¨åæ ‡ç³»ï¼ˆå‡å»ç¯å¢ƒåŸç‚¹åç§»ï¼‰
    object_pos = object_pos_w - env.scene.env_origins

    # ç›®æ ‡ä½ç½®ï¼ˆç¯å¢ƒå±€éƒ¨åæ ‡ç³»ï¼‰
    target_pos = torch.tensor(target_pos_offset, device=env.device).expand(env.num_envs, -1)

    # è®¡ç®—è·ç¦»ï¼ˆåœ¨ç¯å¢ƒå±€éƒ¨åæ ‡ç³»ä¸­ï¼‰
    object_dist = torch.norm(object_pos - target_pos, p=2, dim=-1)

    # ğŸ”¥ æŒ‡æ•°è¡°å‡å¥–åŠ± - é™åˆ¶æŒ‡æ•°å‚æ•°é˜²æ­¢æº¢å‡º
    exp_arg = torch.clamp(-10.0 * object_dist, min=-10.0, max=10.0)
    reward = torch.exp(exp_arg)

    # ğŸ”¥ NaN/Infæ£€æŸ¥
    reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
                        torch.zeros_like(reward), reward)

    return reward


def unstable_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """è®¡ç®—ç¨³å®šæ€§æƒ©ç½š - å‡å°‘ä¸å¿…è¦çš„éœ‡è¡

    Args:
        env: ç¯å¢ƒå®ä¾‹
        object_cfg: ç‰©ä½“èµ„äº§é…ç½®

    Returns:
        ç¨³å®šæ€§æƒ©ç½š (num_envs,)ï¼Œå®é™…ä¸ºè´Ÿçš„æƒ©ç½šé¡¹
        
    Note:
        å¥–åŠ±å…¬å¼ï¼šR = weight * ||v||_2
        å…¶ä¸­væ˜¯ç‰©ä½“çš„çº¿é€Ÿåº¦å‘é‡ï¼Œä½¿ç”¨L2èŒƒæ•°è®¡ç®—é€Ÿåº¦å¤§å°
        weightæ˜¯è´Ÿå·ï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œé€Ÿåº¦è¶Šå¤§æƒ©ç½šè¶Šå¤§
    """
    # è·å–ç‰©ä½“èµ„äº§
    object_asset: RigidObject = env.scene[object_cfg.name]

    # åŸºäºç‰©ä½“çº¿é€Ÿåº¦çš„ç¨³å®šæ€§æƒ©ç½š TODOï¼šä½¿ç”¨è´¨å¿ƒé€Ÿåº¦
    object_lin_vel = object_asset.data.root_lin_vel_w
    # object_lin_vel = object_asset.data.body_com_lin_vel_w.squeeze(-2)
    penalty = torch.norm(object_lin_vel, p=2, dim=-1)

    return penalty


def fall_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    fall_distance: float = 0.12
) -> torch.Tensor:
    """è®¡ç®—æ‰è½æƒ©ç½š

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: ç‰©ä½“èµ„äº§é…ç½®
        fall_distance: æ‰è½è·ç¦»é˜ˆå€¼

    Returns:
        æ‰è½æƒ©ç½š (num_envs,)
    """
    # è·å–ç‰©ä½“èµ„äº§
    asset: RigidObject = env.scene[asset_cfg.name]

    # è·å–ç‰©ä½“ä½ç½®ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
    object_pos_w = asset.data.root_pos_w

    # è½¬æ¢ä¸ºç¯å¢ƒå±€éƒ¨åæ ‡ç³»ï¼ˆå‡å»ç¯å¢ƒåŸç‚¹åç§»ï¼‰
    object_pos = object_pos_w - env.scene.env_origins

    # è·å–ç›®æ ‡ä½ç½®ï¼ˆç¯å¢ƒå±€éƒ¨åæ ‡ç³»ä¸­çš„æ‰‹éƒ¨é™„è¿‘ä½ç½®ï¼‰
    target_pos = torch.tensor([0.0, -0.1, 0.56], device=env.device).expand(env.num_envs, -1)

    # è®¡ç®—è·ç¦»
    distance = torch.norm(object_pos - target_pos, p=2, dim=-1)

    # å¦‚æœè·ç¦»è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›æƒ©ç½š
    return torch.where(distance > fall_distance, torch.ones_like(distance), torch.zeros_like(distance))


def pose_diff_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    natural_pose: dict[str, float] | None = None
) -> torch.Tensor:
    """è®¡ç®—æ‰‹éƒ¨å§¿æ€åå·®æƒ©ç½š - é¼“åŠ±ä¿æŒæ¥è¿‘äººæ‰‹çš„è‡ªç„¶å§¿æ€

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: æœºå™¨äººèµ„äº§é…ç½®
        natural_pose: è‡ªç„¶å§¿æ€çš„å…³èŠ‚è§’åº¦å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼

    Returns:
        å§¿æ€åå·®æƒ©ç½š (num_envs,)
    """
    # è·å–æœºå™¨äººèµ„äº§
    asset: Articulation = env.scene[asset_cfg.name]

    # å®šä¹‰LeapHandçš„è‡ªç„¶å§¿æ€ï¼ˆåŸºäºLEAP_Hand_Isaac_Labé¡¹ç›®çš„å®˜æ–¹é…ç½®ï¼‰
    if natural_pose is None:
        # è¿™äº›å€¼æ¥è‡ªorientation_env.pyä¸­çš„override_default_joint_posé…ç½®
        # æŒ‰ç…§ArticulationDataçš„å…³èŠ‚ç´¢å¼•é¡ºåºï¼ša_0åˆ°a_15
        natural_joint_angles = [
            0.000,  # a_1
            0.500,  # a_12
            0.000,  # a_5
            0.000,  # a_9
            -0.750, # a_0
            1.300,  # a_13
            0.000,  # a_4
            0.750,  # a_8
            1.750,  # a_2
            1.500,  # a_14
            1.750,  # a_6
            1.750,  # a_10
            0.000,  # a_3
            1.000,  # a_15
            0.000,  # a_7
            0.000,  # a_11
        ]

    # å°†è‡ªç„¶å§¿æ€è½¬æ¢ä¸ºå¼ é‡ï¼ˆç›´æ¥æŒ‰å…³èŠ‚ç´¢å¼•é¡ºåºï¼‰
    natural_joint_pos = torch.tensor(
        natural_joint_angles,
        device=env.device,
        dtype=torch.float32
    ).expand(env.num_envs, -1) # ç”¨äºæ‰©å±•å¼ é‡çš„ç»´åº¦ï¼Œå®ƒé€šè¿‡å¤åˆ¶æ•°æ®æ¥åˆ›å»ºä¸€ä¸ªæ›´å¤§çš„è§†å›¾ï¼Œä½†ä¸ä¼šå®é™…åˆ†é…æ–°çš„å†…å­˜

    # è®¡ç®—å½“å‰å…³èŠ‚ä½ç½®ä¸è‡ªç„¶å§¿æ€çš„å·®å¼‚
    current_joint_pos = asset.data.joint_pos
    pose_diff = current_joint_pos - natural_joint_pos

    # è®¡ç®—L2å¹³æ–¹æƒ©ç½š
    pose_diff_penalty = torch.sum(pose_diff ** 2, dim=-1)

    return pose_diff_penalty

###
#  å‚è€ƒLEAP_Hand_Simå¥–åŠ±é¡¹
###

def rotate_angvel_clipped(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    clip_min: float = -0.25,
    clip_max: float = 0.25,
) -> torch.Tensor:
    """è®¡ç®—ç‰©ä½“è§’é€Ÿåº¦åœ¨ç›®æ ‡è½´ä¸Šçš„æŠ•å½±å¹¶è£å‰ªã€‚

    Args:
        env: ManagerBasedRLEnv
            ç¯å¢ƒå®ä¾‹
        asset_cfg: SceneEntityCfg
            ç‰©ä½“èµ„äº§é…ç½®ï¼Œé»˜è®¤ä¸ºSceneEntityCfg("object")
        clip_min: float
            è£å‰ªä¸‹é™ï¼Œé»˜è®¤ä¸º-0.25
        clip_max: float
            è£å‰ªä¸Šé™ï¼Œé»˜è®¤ä¸º0.25

    Returns:
        è£å‰ªåçš„è§’é€Ÿåº¦æŠ•å½±å¥–åŠ± (num_envs,)

    NOTE:
        å¥–åŠ±å…¬å¼ï¼šr = clip(dot(Ï‰_w, Ã¢), clip_min, clip_max)
        å…¶ä¸­:
        - Ï‰_w: ç‰©ä½“åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„è§’é€Ÿåº¦å‘é‡
        - Ã¢: ç›®æ ‡æ—‹è½¬è½´çš„å•ä½å‘é‡
        - clip(): è£å‰ªå‡½æ•°ï¼Œå°†å€¼é™åˆ¶åœ¨[clip_min, clip_max]èŒƒå›´å†…
    """
    object_asset: RigidObject = env.scene[asset_cfg.name]
    ang_vel_w = object_asset.data.root_ang_vel_w  # (N, 3)
    target_axis = env.command_manager.get_command("rotation_axis")  # (N, 3)
    # æŠ•å½±åˆ°ç›®æ ‡è½´ï¼ˆå‡è®¾target_axiså·²å½’ä¸€åŒ–ï¼Œå¦åˆ™éœ€å½’ä¸€åŒ–ï¼‰
    proj = torch.sum(ang_vel_w * target_axis, dim=-1)  # (N,)
    # å¯é€‰ï¼šåªå¥–åŠ±åŒå‘æ—‹è½¬
    proj = torch.where(proj > 0, proj, torch.zeros_like(proj))
    return torch.clamp(proj, min=clip_min, max=clip_max)


def object_linvel_l1_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """è®¡ç®—ç‰©ä½“çº¿é€Ÿåº¦çš„L1èŒƒæ•°æƒ©ç½šã€‚

    Args:
        env: ç¯å¢ƒå®ä¾‹
        object_cfg: ç‰©ä½“èµ„äº§é…ç½®

    Returns:
        çº¿é€Ÿåº¦æƒ©ç½š (num_envs,)

    Note:
        æƒ©ç½šå…¬å¼ï¼šP = âˆ‘|v_w|
        å…¶ä¸­v_wæ˜¯ç‰©ä½“åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„çº¿é€Ÿåº¦å‘é‡
        ä½¿ç”¨L1èŒƒæ•°å¯ä»¥åˆ†åˆ«æƒ©ç½šå„ä¸ªæ–¹å‘çš„é€Ÿåº¦åˆ†é‡
    """
    object_asset: RigidObject = env.scene[object_cfg.name]
    lin_vel_w = object_asset.data.root_lin_vel_w  # (N, 3)
    return torch.sum(torch.abs(lin_vel_w), dim=-1)


def work_penalty_squared(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """æœºæ¢°åŠŸæƒ©ç½šï¼ˆå¹³æ–¹é¡¹ï¼‰

    Args:
        env: ç¯å¢ƒå®ä¾‹
        robot_cfg: æœºå™¨äººèµ„äº§é…ç½®

    Returns:
        æœºæ¢°åŠŸæƒ©ç½š (num_envs,)

    NOTE:
        æ•°å­¦å…¬å¼ï¼š[NOTE: w = (âˆ‘_j Ï„_j Â· qÌ‡_j)^2 ]
        å…¶ä¸­Ï„_jä¸ºç¬¬jä¸ªå…³èŠ‚çš„åŠ›çŸ©ï¼ŒqÌ‡_jä¸ºç¬¬jä¸ªå…³èŠ‚çš„é€Ÿåº¦ã€‚
        è¯¥é¡¹é¼“åŠ±åœ¨å®ç°ç›®æ ‡çš„åŒæ—¶é™ä½åšåŠŸï¼ˆæŠ‘åˆ¶æ— æ•ˆæŒ¤å‹/æŠ–åŠ¨ï¼‰ã€‚
    """
    robot: Articulation = env.scene[robot_cfg.name]
    tau = robot.data.applied_torque[:, robot_cfg.joint_ids]  # (num_envs, DoF)
    qd = robot.data.joint_vel[:, robot_cfg.joint_ids]        # (num_envs, DoF)
    power = torch.sum(tau * qd, dim=-1)  # (num_envs,)
    return torch.square(power)


def object_fall_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    z_threshold: float = 0.10
) -> torch.Tensor:
    """è®¡ç®—ç‰©ä½“æ‰è½æƒ©ç½š - åŸºäºzè½´é«˜åº¦å·®å¼‚åˆ¤æ–­

    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: ç‰©ä½“èµ„äº§é…ç½®
        z_threshold: zè½´é«˜åº¦å·®å¼‚é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼åˆ¤å®šä¸ºæ‰è½

    Returns:
        æ‰è½æƒ©ç½š (num_envs,)ï¼Œæ‰è½æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0

    NOTE:
        æƒ©ç½šå…¬å¼ï¼šP = [|z - z_init| > threshold]
        å…¶ä¸­zæ˜¯å½“å‰ç‰©ä½“é«˜åº¦ï¼Œz_initæ˜¯åˆå§‹é«˜åº¦
        ä½¿ç”¨åˆå§‹ä½ç½®ä½œä¸ºå‚è€ƒï¼Œé¿å…æ‰‹éƒ¨ç§»åŠ¨å¯¼è‡´çš„è¯¯åˆ¤
    """
    # è·å–ç‰©ä½“èµ„äº§
    asset: RigidObject = env.scene[asset_cfg.name]

    # è·å–ç‰©ä½“ä½ç½®ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
    object_pos_w = asset.data.root_pos_w

    # è½¬æ¢ä¸ºç¯å¢ƒå±€éƒ¨åæ ‡ç³»ï¼ˆå‡å»ç¯å¢ƒåŸç‚¹åç§»ï¼‰
    object_pos = object_pos_w - env.scene.env_origins

    # è·å–ç›®æ ‡ä½ç½®ï¼ˆç¯å¢ƒå±€éƒ¨åæ ‡ç³»ä¸­çš„æ‰‹éƒ¨é™„è¿‘ä½ç½®ï¼‰
    inital_pos = asset.cfg.init_state.pos
    target_pos = torch.tensor(inital_pos, device=env.device).expand(env.num_envs, -1)

    # è®¡ç®—è·ç¦»
    dz = torch.abs(object_pos[:, 2] - target_pos[:, 2])

    # å¦‚æœè·ç¦»è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›æƒ©ç½š
    return torch.where(dz > z_threshold, torch.ones_like(dz), torch.zeros_like(dz))


class ContinuousRotationSparseReward:
    """è¿ç»­æ—‹è½¬ç›®æ ‡è¾¾æˆçš„ç¨€ç–å¥–åŠ±ï¼ˆå¯çŠ¶æ€åŒ–å®ç°ï¼‰ã€‚

    å°†åŸæœ¬åœ¨ ``env`` ä¸Šç»´æŠ¤çš„ ``q_last``ã€ç´¯è®¡å¥–åŠ±ç­‰å˜é‡ç§»åŠ¨åˆ°ç±»å®ä¾‹ä¸­ï¼Œ
    ä¾¿äºåœ¨ä¸åŒç¯å¢ƒæˆ–é…ç½®ä¹‹é—´å¤ç”¨åŒä¸€ä¸ªå¥–åŠ±å¯¹è±¡ã€‚
    """

    def __init__(
        self,
        asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
        theta_goal: float = 1.0471975512,
        additive_reward: float = 0.0,
    ) -> None:
        self.asset_cfg = asset_cfg
        self.theta_goal = float(theta_goal)
        self.additive_reward = float(additive_reward)

        self._q_last: torch.Tensor | None = None
        self._next_reward: torch.Tensor | None = None
        self._count: torch.Tensor | None = None
        self._episode_length_prev: torch.Tensor | None = None

    def reset(self, env: ManagerBasedRLEnv, env_ids: torch.Tensor | Sequence[int] | None = None) -> None:
        """æ‰‹åŠ¨é‡ç½®å†…éƒ¨çŠ¶æ€ï¼Œå¯åœ¨ç¯å¢ƒé‡ç½®é’©å­ä¸­è°ƒç”¨ã€‚"""

        if self._q_last is None:
            return

        mask = torch.ones(self._q_last.shape[0], dtype=torch.bool, device=self._q_last.device)
        if env_ids is not None:
            mask = torch.zeros_like(mask)
            env_ids = torch.as_tensor(env_ids, device=self._q_last.device, dtype=torch.long)
            mask[env_ids] = True

        self._apply_reset(env, mask)

    def __call__(
        self,
        env: ManagerBasedRLEnv,
        asset_cfg: SceneEntityCfg | None = None,
        theta_goal: float | None = None,
        additive_reward: float | None = None,
    ) -> torch.Tensor:
        current_asset_cfg = asset_cfg or self.asset_cfg
        current_theta_goal = float(theta_goal if theta_goal is not None else self.theta_goal)
        current_additive_reward = float(
            additive_reward if additive_reward is not None else self.additive_reward
        )

        asset: RigidObject = env.scene[current_asset_cfg.name]
        q_curr = asset.data.root_quat_w

        self.asset_cfg = current_asset_cfg
        self.theta_goal = current_theta_goal
        self.additive_reward = current_additive_reward
        self._ensure_buffers(env, q_curr)

        reset_mask = self._compute_reset_mask(env)
        if reset_mask is not None and torch.any(reset_mask):
            self._apply_reset(env, reset_mask, q_curr=q_curr)

        quat_diff = quat_mul(q_curr, quat_conjugate(self._q_last))  # type: ignore[arg-type]
        w = torch.clamp(torch.abs(quat_diff[:, 0]), 0.0, 1.0)
        theta = 2.0 * torch.acos(w)

        success = theta >= current_theta_goal

        reward = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
        if torch.any(success):
            reward[success] = self._next_reward[success]  # type: ignore[index]
            self._q_last[success] = q_curr[success]  # type: ignore[index]
            self._count[success] += 1  # type: ignore[index]
            self._next_reward[success] = self._next_reward[success] + current_additive_reward  # type: ignore[index]

        return reward

    # ------------------------------------------------------------------
    # å†…éƒ¨è¾…åŠ©æ–¹æ³•
    # ------------------------------------------------------------------
    def _ensure_buffers(self, env: ManagerBasedRLEnv, q_curr: torch.Tensor) -> None:
        if (
            self._q_last is None
            or self._q_last.shape[0] != env.num_envs
            or self._q_last.device != env.device
            or self._q_last.dtype != q_curr.dtype
        ):
            self._q_last = q_curr.clone()
            self._next_reward = torch.ones(env.num_envs, dtype=torch.float32, device=env.device)
            self._count = torch.zeros(env.num_envs, dtype=torch.long, device=env.device)
            self._episode_length_prev = None

    def _compute_reset_mask(self, env: ManagerBasedRLEnv) -> torch.Tensor | None:
        mask: torch.Tensor | None = None

        episode_len = getattr(env, "episode_length_buf", None)
        if episode_len is not None:
            if (
                self._episode_length_prev is None
                or self._episode_length_prev.shape != episode_len.shape
                or self._episode_length_prev.device != episode_len.device
            ):
                mask_episode = torch.ones_like(episode_len, dtype=torch.bool, device=env.device)
                self._episode_length_prev = episode_len.clone()
            else:
                mask_episode = (episode_len == 0) & (self._episode_length_prev != 0)
                self._episode_length_prev.copy_(episode_len)
            mask = mask_episode if mask is None else (mask | mask_episode)

        reset_buf = getattr(env, "reset_buf", None)
        if reset_buf is not None:
            mask_reset = reset_buf.to(device=env.device)
            mask_reset = mask_reset.to(dtype=torch.bool)
            mask = mask_reset if mask is None else (mask | mask_reset)

        return mask

    def _apply_reset(
        self,
        env: ManagerBasedRLEnv,
        mask: torch.Tensor,
        q_curr: torch.Tensor | None = None,
    ) -> None:
        if self._q_last is None or not torch.any(mask):
            return

        if q_curr is None:
            asset: RigidObject = env.scene[self.asset_cfg.name]
            q_curr = asset.data.root_quat_w

        self._q_last[mask] = q_curr[mask]
        self._next_reward[mask] = 1.0
        self._count[mask] = 0

        if self._episode_length_prev is not None and self._episode_length_prev.shape == mask.shape:
            self._episode_length_prev[mask] = 0


# å…¼å®¹æ—§ç‰ˆæ¥å£ï¼šä¿ç•™å¯è°ƒç”¨å®ä¾‹åç§°ã€‚
continuous_rotation_sparse = ContinuousRotationSparseReward()
