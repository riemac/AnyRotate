# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""LeapHandËøûÁª≠ÊóãËΩ¨‰ªªÂä°ÁöÑÂ•ñÂä±ÂáΩÊï∞"""

from __future__ import annotations

from collections.abc import Sequence
from typing import TYPE_CHECKING

import torch

from isaaclab.assets import Articulation, RigidObject
from isaaclab.managers import ManagerTermBase, SceneEntityCfg
from isaaclab.utils.math import quat_conjugate, quat_mul, wrap_to_pi, quat_from_angle_axis
from isaaclab.markers import VisualizationMarkers
from isaaclab.markers.config import BLUE_ARROW_X_MARKER_CFG

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def rotation_velocity(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    visualize_actual_axis: bool = True,
    target_angular_speed: float = 1.5,
    positive_decay: float = 5.0,
    negative_penalty_weight: float = 2.0,
) -> torch.Tensor:
    """ËÆ°ÁÆóÊóãËΩ¨ÈÄüÂ∫¶Â•ñÂä± - ÁõÆÊ†áÊòØËææÂà∞ÊåáÂÆöÁöÑËßíÈÄüÂ∫¶ËÄåÈùûË∂äÂø´Ë∂äÂ•Ω

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        visualize_actual_axis: ÊòØÂê¶ÂèØËßÜÂåñÂÆûÈôÖÊóãËΩ¨ËΩ¥
        target_angular_speed: ÁõÆÊ†áËßíÈÄüÂ∫¶Â§ßÂ∞è (rad/s)
        positive_decay: Ê≠£ÂêëÂ•ñÂä±ÁöÑÊåáÊï∞Ë°∞ÂáèÂõ†Â≠ê
        negative_penalty_weight: Ë¥üÂêëÊÉ©ÁΩöÁöÑÊùÉÈáçÁ≥ªÊï∞

    Returns:
        ÊóãËΩ¨ÈÄüÂ∫¶Â•ñÂä± (num_envs,)

    Note
    -------
        ÊóãËΩ¨ËΩ¥ÊòØÁªïÁöÑ‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏≠ÁöÑÂõ∫ÂÆöËΩ¥ÊóãËΩ¨ÔºåËÄå‰∏çÊòØÁªïÁâ©‰ΩìËá™Ë∫´ÁöÑÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªËΩ¥ÊóãËΩ¨
        Áâ©‰ΩìÊóãËΩ¨Êó∂ÁöÑÊóãËΩ¨ËΩ¥ÂíåBody FrameÁöÑË°®Á§∫Êó†ÂÖ≥
        
        Â•ñÂä±ÂÖ¨ÂºèÔºö
        - Ê≠£ÂêëÈÄüÂ∫¶: R = exp(-positive_decay * |projected_velocity - target_angular_speed|)
        - Ë¥üÂêëÈÄüÂ∫¶: R = negative_penalty_weight * projected_velocity (Ë¥üÊÉ©ÁΩö)
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    asset: RigidObject = env.scene[asset_cfg.name]

    # Ëé∑ÂèñÂΩìÂâçÁâ©‰ΩìÊóãËΩ¨
    current_object_rot = asset.data.root_quat_w # Âõ∫ÂÆöÁöÑ‰∏ñÁïåÂùêÊ†áÁ≥ª

    # ÂàùÂßãÂåñlast_object_rotÂ¶ÇÊûú‰∏çÂ≠òÂú®
    if not hasattr(env, 'last_object_rot'):
        env.last_object_rot = torch.zeros((env.num_envs, 4), dtype=torch.float, device=env.device)
        env.last_object_rot[:, 0] = 1.0  # ÂàùÂßãÂåñ‰∏∫Âçï‰ΩçÂõõÂÖÉÊï∞

    # ËÆ°ÁÆóÊóãËΩ¨Â∑ÆÂºÇ
    quat_diff = quat_mul(current_object_rot, quat_conjugate(env.last_object_rot))
    angle = 2.0 * torch.acos(torch.clamp(torch.abs(quat_diff[:, 0]), max=1.0))

    # ËÆ°ÁÆóÊóãËΩ¨ËΩ¥
    axis = quat_diff[:, 1:4]
    axis_norm = torch.norm(axis, dim=-1, keepdim=True) # ËÆ°ÁÆóÊóãËΩ¨ËΩ¥ÁöÑËåÉÊï∞
    valid_rotation = axis_norm.squeeze(-1) > 1e-6 #  # Âà§Êñ≠ÊòØÂê¶‰∏∫ÊúâÊïàÊóãËΩ¨(ËåÉÊï∞Â§ß‰∫éÈòàÂÄº) omega^hat*thetaÔºåËã•Áâ©‰ΩìÈùôÊ≠¢‰∏çÂä®ÔºåËØ•ËåÉÂºèÂ∞ÜÈùûÂ∏∏Â∞è
    axis = torch.where(valid_rotation.unsqueeze(-1), axis / axis_norm, torch.zeros_like(axis)) # ÂØπÊúâÊïàÊóãËΩ¨ËøõË°åÂΩí‰∏ÄÂåñ,Êó†ÊïàÊóãËΩ¨ÁΩÆÈõ∂

    # Ëé∑ÂèñÁõÆÊ†áÊóãËΩ¨ËΩ¥ - ‰ªéCommandÁÆ°ÁêÜÂô®Ëé∑Âèñ
    rotation_axis = env.command_manager.get_command("rotation_axis")

    # ËÆ°ÁÆóÊ≤øÊåáÂÆöÊóãËΩ¨ËΩ¥ÁöÑËßíÈÄüÂ∫¶
    angular_velocity = angle / env.step_dt
    projected_velocity = torch.sum(axis * rotation_axis, dim=-1) * angular_velocity

    # Êõ¥Êñ∞‰∏ä‰∏ÄÂ∏ßÊóãËΩ¨
    env.last_object_rot[:] = current_object_rot.clone()

    # --- Ê†∏ÂøÉ‰øÆÊîπÔºö‰ΩøÁî®Â•ñÊÉ©‰∏Ä‰ΩìÁöÑÈÄªËæë ---
    # 1. ÂØπ‰∫éÊ≠£ÂêëÈÄüÂ∫¶ (ÊñπÂêëÊ≠£Á°Æ)
    #    Êàë‰ª¨‰ΩøÁî®ÊåáÊï∞Ë°∞ÂáèÂΩ¢ÂºèÔºåÈºìÂä±ÈÄºËøëÁõÆÊ†áÈÄüÂ∫¶
    speed_error_positive = torch.abs(projected_velocity - target_angular_speed)
    # üî• ÈôêÂà∂ÊåáÊï∞ÂèÇÊï∞ÔºåÈò≤Ê≠¢exp()Ê∫¢Âá∫
    exp_arg = torch.clamp(-positive_decay * speed_error_positive, min=-10.0, max=10.0)
    reward_positive = torch.exp(exp_arg)

    # 2. ÂØπ‰∫éË¥üÂêëÈÄüÂ∫¶ (ÊñπÂêëÈîôËØØ)
    #    Êàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™Á∫øÊÄßÁöÑÊÉ©ÁΩöÈ°π„ÄÇÈÄüÂ∫¶Ë∂äË¥üÔºåÊÉ©ÁΩöË∂äÂ§ß„ÄÇ
    #    projected_velocityÊòØË¥üÁöÑÔºåÊâÄ‰ª•‰πò‰ª•‰∏Ä‰∏™Ê≠£ÊùÉÈáçÂ∞±ÂèòÊàê‰∫ÜË¥üÁöÑÂ•ñÂä±ÔºàÊÉ©ÁΩöÔºâ
    reward_negative = negative_penalty_weight * projected_velocity

    # 3. ‰ΩøÁî® torch.where Ê†πÊçÆÈÄüÂ∫¶ÊñπÂêëÈÄâÊã©Â•ñÂä±/ÊÉ©ÁΩö
    #    ÂΩì projected_velocity > 0 Êó∂ÔºåÈááÁî® reward_positive
    #    Âê¶Âàô (<= 0)ÔºåÈááÁî® reward_negative
    reward = torch.where(
        projected_velocity > 0,
        reward_positive,
        reward_negative
    )

    # üî• ÊúÄÁªàÁöÑNaN/InfÊ£ÄÊü•
    # reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
    #                     torch.zeros_like(reward), reward)

    # ÂèØËßÜÂåñÂÆûÈôÖÊóãËΩ¨ËΩ¥
    if visualize_actual_axis:
        _visualize_actual_rotation_axis(env, asset, axis, valid_rotation)

    return reward


def _visualize_actual_rotation_axis(
    env: ManagerBasedRLEnv,
    asset: RigidObject,
    actual_axis: torch.Tensor,
    valid_rotation: torch.Tensor,
):
    """ÂèØËßÜÂåñÂÆûÈôÖÊóãËΩ¨ËΩ¥

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset: Áâ©‰ΩìËµÑ‰∫ß
        actual_axis: ÂÆûÈôÖÊóãËΩ¨ËΩ¥ (num_envs, 3)
        valid_rotation: ÊúâÊïàÊóãËΩ¨Êé©Á†Å (num_envs,)
    """
    # ÂàùÂßãÂåñÂèØËßÜÂåñÂô®ÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ
    if not hasattr(env, '_actual_axis_visualizer'):
        # ÂàõÂª∫ËìùËâ≤ÁÆ≠Â§¥ÂèØËßÜÂåñÂô®
        marker_cfg = BLUE_ARROW_X_MARKER_CFG.replace(
            prim_path="/Visuals/Reward/actual_rotation_axis"
        )
        # ËÆæÁΩÆÁÆ≠Â§¥Â∞∫ÂØ∏Ôºà‰∏éÁõÆÊ†áËΩ¥Áõ∏ÂêåÔºâ
        marker_cfg.markers["arrow"].scale = (0.1, 0.1, 0.3)
        env._actual_axis_visualizer = VisualizationMarkers(marker_cfg)

    # Âè™ÊòæÁ§∫ÊúâÊïàÊóãËΩ¨ÁöÑÁÆ≠Â§¥
    valid_env_ids = valid_rotation.nonzero(as_tuple=False).squeeze(-1)
    if len(valid_env_ids) == 0:
        return

    # Ëé∑ÂèñÁâ©‰Ωì‰ΩçÁΩÆ
    object_pos_w = asset.data.root_pos_w[valid_env_ids]

    # ËÆ°ÁÆóÁÆ≠Â§¥‰ΩçÁΩÆÔºàÁâ©‰Ωì‰∏äÊñπÔºå‰ΩÜ‰∏éÁõÆÊ†áËΩ¥Êúâ‰∏çÂêåÂÅèÁßªÈÅøÂÖçÈáçÂè†Ôºâ
    arrow_positions = object_pos_w.clone()
    arrow_positions[:, 2] += 0.20  # ÊØîÁõÆÊ†áËΩ¥Á®çÈ´ò‰∏Ä‰∫õÔºàÁõÆÊ†áËΩ¥ÊòØ0.15Ôºâ

    # Áõ¥Êé•‰ΩøÁî®ÂÆûÈôÖÊóãËΩ¨ËΩ¥ËÆ°ÁÆóÁÆ≠Â§¥ÊñπÂêë
    valid_axes = actual_axis[valid_env_ids]
    arrow_orientations = _compute_arrow_orientations_from_axis(valid_axes, env.device)

    # ÂàõÂª∫marker_indices
    marker_indices = torch.zeros(len(valid_env_ids), device=env.device, dtype=torch.int32)

    # Êõ¥Êñ∞ÂèØËßÜÂåñ
    env._actual_axis_visualizer.visualize(
        translations=arrow_positions,
        orientations=arrow_orientations,
        marker_indices=marker_indices
    )


def _compute_arrow_orientations_from_axis(axis: torch.Tensor, device: torch.device) -> torch.Tensor:
    """‰ªéÊóãËΩ¨ËΩ¥ËÆ°ÁÆóÁÆ≠Â§¥ÊñπÂêëÂõõÂÖÉÊï∞

    Args:
        axis: Â∑≤ÂΩí‰∏ÄÂåñÁöÑÊóãËΩ¨ËΩ¥ÂêëÈáè (num_envs, 3)
        device: ËÆæÂ§á

    Returns:
        ÁÆ≠Â§¥ÊñπÂêëÂõõÂÖÉÊï∞ (num_envs, 4) - (w, x, y, z)

    Note:
        ËæìÂÖ•ÁöÑaxisÂ∑≤ÁªèÂú®rotation_velocity_reward‰∏≠Ë¢´ÂΩí‰∏ÄÂåñÔºåÊó†ÈúÄÈáçÂ§çÂΩí‰∏ÄÂåñ
    """
    # ÁÆ≠Â§¥ÈªòËÆ§ÊñπÂêëÔºàXËΩ¥Ê≠£ÊñπÂêëÔºâ
    default_direction = torch.tensor([1.0, 0.0, 0.0], device=device)

    # ËÆ°ÁÆóÊóãËΩ¨ËΩ¥ÔºàÂèâÁßØÔºâÂíåËßíÂ∫¶ÔºàÁÇπÁßØÔºâ
    rotation_axis = torch.cross(default_direction.unsqueeze(0).expand_as(axis), axis, dim=-1)
    cos_angle = torch.sum(default_direction.unsqueeze(0) * axis, dim=-1)

    # ËÆ°ÁÆóÊóãËΩ¨ËßíÂ∫¶
    angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0))

    # Â§ÑÁêÜÊóãËΩ¨ËΩ¥‰∏∫Èõ∂ÁöÑÊÉÖÂÜµÔºàÂêëÈáèÂπ≥Ë°åÔºâ
    rotation_axis_norm = torch.norm(rotation_axis, dim=-1, keepdim=True)
    rotation_axis = torch.where(
        rotation_axis_norm > 1e-6,
        rotation_axis / rotation_axis_norm,
        torch.tensor([0.0, 0.0, 1.0], device=device).unsqueeze(0).expand_as(rotation_axis)
    )

    # ‰ΩøÁî®Isaac LabÂÆòÊñπÂáΩÊï∞ËÆ°ÁÆóÂõõÂÖÉÊï∞
    orientations = quat_from_angle_axis(angle, rotation_axis)

    return orientations


def fingertip_distance_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """ËÆ°ÁÆóÊåáÂ∞ñÂà∞Áâ©‰Ωì‰∏≠ÂøÉË∑ùÁ¶ªÁöÑÊÉ©ÁΩö - ÈºìÂä±ÊåáÂ∞ñÊé•ËøëÁâ©‰Ωì

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        object_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        robot_cfg: Êú∫Âô®‰∫∫ËµÑ‰∫ßÈÖçÁΩÆ

    Returns:
        ÊåáÂ∞ñË∑ùÁ¶ªÊÉ©ÁΩö (num_envs,)
        
    Note:
        Â•ñÂä±ÂÖ¨ÂºèÔºöR = mean(||fingertip_pos - object_pos||_2)
        ÂÖ∂‰∏≠fingertip_posÊòØÊØè‰∏™ÊåáÂ∞ñÁöÑ‰∏ñÁïåÂùêÊ†á‰ΩçÁΩÆÔºåobject_posÊòØÁâ©‰ΩìË¥®ÂøÉÁöÑ‰∏ñÁïåÂùêÊ†á‰ΩçÁΩÆ
        ‰ΩøÁî®Âπ≥ÂùáË∑ùÁ¶ªËÄå‰∏çÊòØÊúÄÂ∞èË∑ùÁ¶ªÂèØ‰ª•ËÆ©ÊâÄÊúâÊâãÊåáÈÉΩÂèÇ‰∏éÊäìÂèñÔºåÈÅøÂÖçÈÉ®ÂàÜÊâãÊåá‰∏çÊìç‰Ωú
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    object_asset: RigidObject = env.scene[object_cfg.name]
    robot_asset: Articulation = env.scene[robot_cfg.name]

    # Ëé∑ÂèñÁâ©‰ΩìË¥®Èáè‰∏≠ÂøÉ‰ΩçÁΩÆ
    object_pos_w = object_asset.data.root_pos_w

    # LeapHandÊåáÂ∞ñbodyÂêçÁß∞ÔºàÂü∫‰∫éÂÆûÈôÖÁöÑbody_namesËæìÂá∫Ôºâ
    fingertip_body_names = [
        "fingertip",         # È£üÊåáÊåáÂ∞ñ
        "thumb_fingertip",   # ÊãáÊåáÊåáÂ∞ñ
        "fingertip_2",       # ‰∏≠ÊåáÊåáÂ∞ñ
        "fingertip_3"        # Êó†ÂêçÊåáÊåáÂ∞ñ
    ]

    # Ëé∑ÂèñÊâÄÊúâÊåáÂ∞ñÁöÑ‰ΩçÁΩÆ
    fingertip_distances = []
    
    for body_name in fingertip_body_names:
        # Ëé∑ÂèñÊåáÂ∞ñbodyÁöÑ‰∏ñÁïåÂùêÊ†á‰ΩçÁΩÆ
        body_indices, _ = robot_asset.find_bodies(body_name)
        # Ëã•Êó†ÂåπÈÖçÔºåÁõ¥Êé•ÊäõÂá∫ÂºÇÂ∏∏
        if len(body_indices) == 0:
            raise IndexError(f"Body not found: {body_name}")
        # ‰ΩøÁî®Á¨¨‰∏Ä‰∏™ÂåπÈÖçÂà∞ÁöÑÁ¥¢ÂºïÔºàPython intÔºâ
        body_idx = int(body_indices[0])        
        fingertip_pos_w = robot_asset.data.body_pos_w[:, body_idx]
        
        # ËÆ°ÁÆóÊåáÂ∞ñÂà∞Áâ©‰Ωì‰∏≠ÂøÉÁöÑË∑ùÁ¶ª
        distance = torch.norm(fingertip_pos_w - object_pos_w, p=2, dim=-1)
        fingertip_distances.append(distance)

    # Â∞ÜÊâÄÊúâÊåáÂ∞ñË∑ùÁ¶ªÂ†ÜÂè†‰∏∫Âº†Èáè (num_envs, num_fingertips)
    fingertip_distances_tensor = torch.stack(fingertip_distances, dim=-1)
    # ËÆ°ÁÆóÊúÄÂ∞èË∑ùÁ¶ªÔºàÊúÄÊé•ËøëÁâ©‰ΩìÁöÑÊåáÂ∞ñÔºâÊàñÂπ≥ÂùáË∑ùÁ¶ªÔºàÊõ¥Âπ≥ÊªëÔºâ
    # min_distance = torch.min(fingertip_distances_tensor, dim=-1)[0]
    distance = torch.mean(fingertip_distances_tensor, dim=-1)
    # ËøîÂõûÂΩ¢Áä∂ (num_envs,)
    return distance


def rotation_axis_alignment_reward(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    theta_tolerance: float = 0.1,
    decay_factor: float = 5.0,
) -> torch.Tensor:
    """ËÆ°ÁÆóÊóãËΩ¨ËΩ¥ÂÆπÂøçÂ∑ÆÂ•ñÂä± - ÊåáÊï∞Ë°∞ÂáèÂûãÂ•ñÂä±

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        theta_tolerance: ËßíÂ∫¶ÂÆπÂøçÂ∫¶ (ÂºßÂ∫¶)
        decay_factor: ÊåáÊï∞Ë°∞ÂáèÂõ†Â≠ê

    Returns:
        ÊóãËΩ¨ËΩ¥ÂØπÈΩêÂ•ñÂä± (num_envs,)

    Note:
        Â•ñÂä±ÂÖ¨ÂºèÔºöR_axis = weight * exp(-decay_factor * max(0, theta - theta_tolerance))
        ÂÖ∂‰∏≠thetaÊòØÂÆûÈôÖÊóãËΩ¨ËΩ¥‰∏éÁõÆÊ†áÊóãËΩ¨ËΩ¥‰πãÈó¥ÁöÑÂ§πËßí
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    asset: RigidObject = env.scene[asset_cfg.name]

    # Ëé∑ÂèñÂΩìÂâçÁâ©‰ΩìÊóãËΩ¨
    current_object_rot = asset.data.root_quat_w

    # ÂàùÂßãÂåñÁã¨Á´ãÁöÑlast_object_rotÁä∂ÊÄÅÔºàÈÅøÂÖç‰∏érotation_velocity_rewardÂÜ≤Á™ÅÔºâ
    if not hasattr(env, 'last_object_rot_alignment'):
        env.last_object_rot_alignment = torch.zeros((env.num_envs, 4), dtype=torch.float, device=env.device)
        env.last_object_rot_alignment[:, 0] = 1.0  # ÂàùÂßãÂåñ‰∏∫Âçï‰ΩçÂõõÂÖÉÊï∞

    # ËÆ°ÁÆóÊóãËΩ¨Â∑ÆÂºÇ
    quat_diff = quat_mul(current_object_rot, quat_conjugate(env.last_object_rot_alignment))

    # ËÆ°ÁÆóÂÆûÈôÖÊóãËΩ¨ËΩ¥
    axis = quat_diff[:, 1:4]
    axis_norm = torch.norm(axis, dim=-1, keepdim=True)
    valid_rotation = axis_norm.squeeze(-1) > 1e-6
    axis = torch.where(valid_rotation.unsqueeze(-1), axis / axis_norm, torch.zeros_like(axis))

    # Ëé∑ÂèñÁõÆÊ†áÊóãËΩ¨ËΩ¥
    target_axis = env.command_manager.get_command("rotation_axis")

    # ËÆ°ÁÆóÂÆûÈôÖÊóãËΩ¨ËΩ¥‰∏éÁõÆÊ†áÊóãËΩ¨ËΩ¥‰πãÈó¥ÁöÑÂ§πËßí
    # ‰ΩøÁî®ÁÇπÁßØËÆ°ÁÆóÂ§πËßíÔºöcos(theta) = a¬∑b / (|a||b|)
    dot_product = torch.sum(axis * target_axis, dim=-1)
    # üî• Êõ¥‰∏•Ê†ºÁöÑÊï∞ÂÄºÁ®≥ÂÆöÊÄßÂ§ÑÁêÜ
    dot_product = torch.clamp(dot_product, -0.9999, 0.9999)  # ÈÅøÂÖçacos(¬±1)ÁöÑÊï∞ÂÄºÈóÆÈ¢ò

    # üî• ÂÆâÂÖ®ÁöÑËßíÂ∫¶ËÆ°ÁÆóÔºåÂ§ÑÁêÜÊó†ÊïàÊóãËΩ¨ÁöÑÊÉÖÂÜµ
    theta = torch.where(
        valid_rotation,
        torch.acos(torch.abs(dot_product)),  # Âè™ËÆ°ÁÆóÁªùÂØπÂÄºÁöÑËßíÂ∫¶ÔºåÈÅøÂÖçÊñπÂêëÈóÆÈ¢ò
        torch.zeros_like(dot_product)        # Êó†ÊïàÊóãËΩ¨Êó∂ËßíÂ∫¶‰∏∫0
    )

    # ËÆ°ÁÆóÊåáÊï∞Ë°∞ÂáèÂ•ñÂä±
    angle_error = torch.clamp(theta - theta_tolerance, min=0.0)
    reward = torch.exp(-decay_factor * angle_error)

    # üî• È¢ùÂ§ñÁöÑNaNÊ£ÄÊü•ÂíåÂ§ÑÁêÜ
    reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
                        torch.zeros_like(reward), reward)

    # Êõ¥Êñ∞‰∏ä‰∏ÄÂ∏ßÊóãËΩ¨ÔºàÁã¨Á´ãÁä∂ÊÄÅÔºâ
    env.last_object_rot_alignment[:] = current_object_rot.clone()

    return reward


def grasp_reward(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    target_pos_offset: tuple[float, float, float] = (0.0, -0.1, 0.56)
) -> torch.Tensor:
    """ËÆ°ÁÆóÊäìÂèñÂ•ñÂä± - ÈºìÂä±‰øùÊåÅÁâ©‰ΩìÂú®Êâã‰∏≠

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        object_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        target_pos_offset: ÁõÆÊ†á‰ΩçÁΩÆÂÅèÁßªÔºàÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÔºâ

    Returns:
        ÊäìÂèñÂ•ñÂä± (num_envs,)
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    object_asset: RigidObject = env.scene[object_cfg.name]

    # Ëé∑ÂèñÁâ©‰Ωì‰ΩçÁΩÆÔºà‰∏ñÁïåÂùêÊ†áÁ≥ªÔºâ
    object_pos_w = object_asset.data.root_pos_w

    # ËΩ¨Êç¢‰∏∫ÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÔºàÂáèÂéªÁéØÂ¢ÉÂéüÁÇπÂÅèÁßªÔºâ
    object_pos = object_pos_w - env.scene.env_origins

    # ÁõÆÊ†á‰ΩçÁΩÆÔºàÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÔºâ
    target_pos = torch.tensor(target_pos_offset, device=env.device).expand(env.num_envs, -1)

    # ËÆ°ÁÆóË∑ùÁ¶ªÔºàÂú®ÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠Ôºâ
    object_dist = torch.norm(object_pos - target_pos, p=2, dim=-1)

    # üî• ÊåáÊï∞Ë°∞ÂáèÂ•ñÂä± - ÈôêÂà∂ÊåáÊï∞ÂèÇÊï∞Èò≤Ê≠¢Ê∫¢Âá∫
    exp_arg = torch.clamp(-10.0 * object_dist, min=-10.0, max=10.0)
    reward = torch.exp(exp_arg)

    # üî• NaN/InfÊ£ÄÊü•
    reward = torch.where(torch.isnan(reward) | torch.isinf(reward),
                        torch.zeros_like(reward), reward)

    return reward


def unstable_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """ËÆ°ÁÆóÁ®≥ÂÆöÊÄßÊÉ©ÁΩö - ÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÈúáËç°

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        object_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ

    Returns:
        Á®≥ÂÆöÊÄßÊÉ©ÁΩö (num_envs,)ÔºåÂÆûÈôÖ‰∏∫Ë¥üÁöÑÊÉ©ÁΩöÈ°π
        
    Note:
        Â•ñÂä±ÂÖ¨ÂºèÔºöR = weight * ||v||_2
        ÂÖ∂‰∏≠vÊòØÁâ©‰ΩìÁöÑÁ∫øÈÄüÂ∫¶ÂêëÈáèÔºå‰ΩøÁî®L2ËåÉÊï∞ËÆ°ÁÆóÈÄüÂ∫¶Â§ßÂ∞è
        weightÊòØË¥üÂè∑ÔºåË°®Á§∫ËøôÊòØ‰∏Ä‰∏™ÊÉ©ÁΩöÈ°πÔºåÈÄüÂ∫¶Ë∂äÂ§ßÊÉ©ÁΩöË∂äÂ§ß
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    object_asset: RigidObject = env.scene[object_cfg.name]

    # Âü∫‰∫éÁâ©‰ΩìÁ∫øÈÄüÂ∫¶ÁöÑÁ®≥ÂÆöÊÄßÊÉ©ÁΩö TODOÔºö‰ΩøÁî®Ë¥®ÂøÉÈÄüÂ∫¶
    object_lin_vel = object_asset.data.root_lin_vel_w
    # object_lin_vel = object_asset.data.body_com_lin_vel_w.squeeze(-2)
    penalty = torch.norm(object_lin_vel, p=2, dim=-1)

    return penalty


def fall_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    fall_distance: float = 0.12
) -> torch.Tensor:
    """ËÆ°ÁÆóÊéâËêΩÊÉ©ÁΩö

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        fall_distance: ÊéâËêΩË∑ùÁ¶ªÈòàÂÄº

    Returns:
        ÊéâËêΩÊÉ©ÁΩö (num_envs,)
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    asset: RigidObject = env.scene[asset_cfg.name]

    # Ëé∑ÂèñÁâ©‰Ωì‰ΩçÁΩÆÔºà‰∏ñÁïåÂùêÊ†áÁ≥ªÔºâ
    object_pos_w = asset.data.root_pos_w

    # ËΩ¨Êç¢‰∏∫ÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÔºàÂáèÂéªÁéØÂ¢ÉÂéüÁÇπÂÅèÁßªÔºâ
    object_pos = object_pos_w - env.scene.env_origins

    # Ëé∑ÂèñÁõÆÊ†á‰ΩçÁΩÆÔºàÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠ÁöÑÊâãÈÉ®ÈôÑËøë‰ΩçÁΩÆÔºâ
    target_pos = torch.tensor([0.0, -0.1, 0.56], device=env.device).expand(env.num_envs, -1)

    # ËÆ°ÁÆóË∑ùÁ¶ª
    distance = torch.norm(object_pos - target_pos, p=2, dim=-1)

    # Â¶ÇÊûúË∑ùÁ¶ªË∂ÖËøáÈòàÂÄºÔºåËøîÂõûÊÉ©ÁΩö
    return torch.where(distance > fall_distance, torch.ones_like(distance), torch.zeros_like(distance))


def pose_diff_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    natural_pose: dict[str, float] | None = None
) -> torch.Tensor:
    """ËÆ°ÁÆóÊâãÈÉ®ÂßøÊÄÅÂÅèÂ∑ÆÊÉ©ÁΩö - ÈºìÂä±‰øùÊåÅÊé•Ëøë‰∫∫ÊâãÁöÑËá™ÁÑ∂ÂßøÊÄÅ

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: Êú∫Âô®‰∫∫ËµÑ‰∫ßÈÖçÁΩÆ
        natural_pose: Ëá™ÁÑ∂ÂßøÊÄÅÁöÑÂÖ≥ËäÇËßíÂ∫¶Â≠óÂÖ∏ÔºåÂ¶ÇÊûú‰∏∫NoneÂàô‰ΩøÁî®ÈªòËÆ§ÂÄº

    Returns:
        ÂßøÊÄÅÂÅèÂ∑ÆÊÉ©ÁΩö (num_envs,)
    """
    # Ëé∑ÂèñÊú∫Âô®‰∫∫ËµÑ‰∫ß
    asset: Articulation = env.scene[asset_cfg.name]

    # ÂÆö‰πâLeapHandÁöÑËá™ÁÑ∂ÂßøÊÄÅÔºàÂü∫‰∫éLEAP_Hand_Isaac_LabÈ°πÁõÆÁöÑÂÆòÊñπÈÖçÁΩÆÔºâ
    if natural_pose is None:
        # Ëøô‰∫õÂÄºÊù•Ëá™orientation_env.py‰∏≠ÁöÑoverride_default_joint_posÈÖçÁΩÆ
        # ÊåâÁÖßArticulationDataÁöÑÂÖ≥ËäÇÁ¥¢ÂºïÈ°∫Â∫èÔºöa_0Âà∞a_15
        natural_joint_angles = [
            0.000,  # a_1
            0.500,  # a_12
            0.000,  # a_5
            0.000,  # a_9
            -0.750, # a_0
            1.300,  # a_13
            0.000,  # a_4
            0.750,  # a_8
            1.750,  # a_2
            1.500,  # a_14
            1.750,  # a_6
            1.750,  # a_10
            0.000,  # a_3
            1.000,  # a_15
            0.000,  # a_7
            0.000,  # a_11
        ]

    # Â∞ÜËá™ÁÑ∂ÂßøÊÄÅËΩ¨Êç¢‰∏∫Âº†ÈáèÔºàÁõ¥Êé•ÊåâÂÖ≥ËäÇÁ¥¢ÂºïÈ°∫Â∫èÔºâ
    natural_joint_pos = torch.tensor(
        natural_joint_angles,
        device=env.device,
        dtype=torch.float32
    ).expand(env.num_envs, -1) # Áî®‰∫éÊâ©Â±ïÂº†ÈáèÁöÑÁª¥Â∫¶ÔºåÂÆÉÈÄöËøáÂ§çÂà∂Êï∞ÊçÆÊù•ÂàõÂª∫‰∏Ä‰∏™Êõ¥Â§ßÁöÑËßÜÂõæÔºå‰ΩÜ‰∏ç‰ºöÂÆûÈôÖÂàÜÈÖçÊñ∞ÁöÑÂÜÖÂ≠ò

    # ËÆ°ÁÆóÂΩìÂâçÂÖ≥ËäÇ‰ΩçÁΩÆ‰∏éËá™ÁÑ∂ÂßøÊÄÅÁöÑÂ∑ÆÂºÇ
    current_joint_pos = asset.data.joint_pos
    pose_diff = current_joint_pos - natural_joint_pos

    # ËÆ°ÁÆóL2Âπ≥ÊñπÊÉ©ÁΩö
    pose_diff_penalty = torch.sum(pose_diff ** 2, dim=-1)

    return pose_diff_penalty

###
#  ÂèÇËÄÉLEAP_Hand_SimÂ•ñÂä±È°π
###

def rotate_angvel_clipped(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    clip_min: float = -0.25,
    clip_max: float = 0.25,
) -> torch.Tensor:
    """ËÆ°ÁÆóÁâ©‰ΩìËßíÈÄüÂ∫¶Âú®ÁõÆÊ†áËΩ¥‰∏äÁöÑÊäïÂΩ±Âπ∂Ë£ÅÂâ™„ÄÇ

    Args:
        env: ManagerBasedRLEnv
            ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: SceneEntityCfg
            Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫SceneEntityCfg("object")
        clip_min: float
            Ë£ÅÂâ™‰∏ãÈôêÔºåÈªòËÆ§‰∏∫-0.25
        clip_max: float
            Ë£ÅÂâ™‰∏äÈôêÔºåÈªòËÆ§‰∏∫0.25

    Returns:
        Ë£ÅÂâ™ÂêéÁöÑËßíÈÄüÂ∫¶ÊäïÂΩ±Â•ñÂä± (num_envs,)

    NOTE:
        Â•ñÂä±ÂÖ¨ÂºèÔºör = clip(dot(œâ_w, √¢), clip_min, clip_max)
        ÂÖ∂‰∏≠:
        - œâ_w: Áâ©‰ΩìÂú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑËßíÈÄüÂ∫¶ÂêëÈáè
        - √¢: ÁõÆÊ†áÊóãËΩ¨ËΩ¥ÁöÑÂçï‰ΩçÂêëÈáè
        - clip(): Ë£ÅÂâ™ÂáΩÊï∞ÔºåÂ∞ÜÂÄºÈôêÂà∂Âú®[clip_min, clip_max]ËåÉÂõ¥ÂÜÖ
    """
    object_asset: RigidObject = env.scene[asset_cfg.name]
    ang_vel_w = object_asset.data.root_ang_vel_w  # (N, 3)
    target_axis = env.command_manager.get_command("rotation_axis")  # (N, 3)
    # ÊäïÂΩ±Âà∞ÁõÆÊ†áËΩ¥ÔºàÂÅáËÆætarget_axisÂ∑≤ÂΩí‰∏ÄÂåñÔºåÂê¶ÂàôÈúÄÂΩí‰∏ÄÂåñÔºâ
    proj = torch.sum(ang_vel_w * target_axis, dim=-1)  # (N,)
    # ÂèØÈÄâÔºöÂè™Â•ñÂä±ÂêåÂêëÊóãËΩ¨
    proj = torch.where(proj > 0, proj, torch.zeros_like(proj))
    return torch.clamp(proj, min=clip_min, max=clip_max)


def object_linvel_l1_penalty(
    env: ManagerBasedRLEnv,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """ËÆ°ÁÆóÁâ©‰ΩìÁ∫øÈÄüÂ∫¶ÁöÑL1ËåÉÊï∞ÊÉ©ÁΩö„ÄÇ

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        object_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ

    Returns:
        Á∫øÈÄüÂ∫¶ÊÉ©ÁΩö (num_envs,)

    Note:
        ÊÉ©ÁΩöÂÖ¨ÂºèÔºöP = ‚àë|v_w|
        ÂÖ∂‰∏≠v_wÊòØÁâ©‰ΩìÂú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑÁ∫øÈÄüÂ∫¶ÂêëÈáè
        ‰ΩøÁî®L1ËåÉÊï∞ÂèØ‰ª•ÂàÜÂà´ÊÉ©ÁΩöÂêÑ‰∏™ÊñπÂêëÁöÑÈÄüÂ∫¶ÂàÜÈáè
    """
    object_asset: RigidObject = env.scene[object_cfg.name]
    lin_vel_w = object_asset.data.root_lin_vel_w  # (N, 3)
    return torch.sum(torch.abs(lin_vel_w), dim=-1)


def work_penalty_squared(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """Êú∫Ê¢∞ÂäüÊÉ©ÁΩöÔºàÂπ≥ÊñπÈ°πÔºâ

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        robot_cfg: Êú∫Âô®‰∫∫ËµÑ‰∫ßÈÖçÁΩÆ

    Returns:
        Êú∫Ê¢∞ÂäüÊÉ©ÁΩö (num_envs,)

    Note:
        Êï∞Â≠¶ÂÖ¨ÂºèÔºöw = (‚àë_j œÑ_j ¬∑ qÃá_j)^2 
        ÂÖ∂‰∏≠œÑ_j‰∏∫Á¨¨j‰∏™ÂÖ≥ËäÇÁöÑÂäõÁü©ÔºåqÃá_j‰∏∫Á¨¨j‰∏™ÂÖ≥ËäÇÁöÑÈÄüÂ∫¶„ÄÇ
        ËØ•È°πÈºìÂä±Âú®ÂÆûÁé∞ÁõÆÊ†áÁöÑÂêåÊó∂Èôç‰ΩéÂÅöÂäüÔºàÊäëÂà∂Êó†ÊïàÊå§Âéã/ÊäñÂä®Ôºâ„ÄÇ
    """
    robot: Articulation = env.scene[robot_cfg.name]
    tau = robot.data.applied_torque[:, robot_cfg.joint_ids]  # (num_envs, DoF)
    qd = robot.data.joint_vel[:, robot_cfg.joint_ids]        # (num_envs, DoF)
    power = torch.sum(tau * qd, dim=-1)  # (num_envs,)
    return torch.square(power)


def object_fall_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    z_threshold: float = 0.10
) -> torch.Tensor:
    """ËÆ°ÁÆóÁâ©‰ΩìÊéâËêΩÊÉ©ÁΩö - Âü∫‰∫ézËΩ¥È´òÂ∫¶Â∑ÆÂºÇÂà§Êñ≠

    Args:
        env: ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg: Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆ
        z_threshold: zËΩ¥È´òÂ∫¶Â∑ÆÂºÇÈòàÂÄºÔºåË∂ÖËøáÊ≠§ÂÄºÂà§ÂÆö‰∏∫ÊéâËêΩ

    Returns:
        ÊéâËêΩÊÉ©ÁΩö (num_envs,)ÔºåÊéâËêΩÊó∂‰∏∫1ÔºåÂê¶Âàô‰∏∫0

    Note:
        ÊÉ©ÁΩöÂÖ¨ÂºèÔºöP = [|z - z_init| > threshold]
        ÂÖ∂‰∏≠zÊòØÂΩìÂâçÁâ©‰ΩìÈ´òÂ∫¶Ôºåz_initÊòØÂàùÂßãÈ´òÂ∫¶
        ‰ΩøÁî®ÂàùÂßã‰ΩçÁΩÆ‰Ωú‰∏∫ÂèÇËÄÉÔºåÈÅøÂÖçÊâãÈÉ®ÁßªÂä®ÂØºËá¥ÁöÑËØØÂà§
    """
    # Ëé∑ÂèñÁâ©‰ΩìËµÑ‰∫ß
    asset: RigidObject = env.scene[asset_cfg.name]

    # Ëé∑ÂèñÁâ©‰Ωì‰ΩçÁΩÆÔºà‰∏ñÁïåÂùêÊ†áÁ≥ªÔºâ
    object_pos_w = asset.data.root_pos_w

    # ËΩ¨Êç¢‰∏∫ÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÔºàÂáèÂéªÁéØÂ¢ÉÂéüÁÇπÂÅèÁßªÔºâ
    object_pos = object_pos_w - env.scene.env_origins

    # Ëé∑ÂèñÁõÆÊ†á‰ΩçÁΩÆÔºàÁéØÂ¢ÉÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠ÁöÑÊâãÈÉ®ÈôÑËøë‰ΩçÁΩÆÔºâ
    inital_pos = asset.cfg.init_state.pos
    target_pos = torch.tensor(inital_pos, device=env.device).expand(env.num_envs, -1)

    # ËÆ°ÁÆóË∑ùÁ¶ª
    dz = torch.abs(object_pos[:, 2] - target_pos[:, 2])

    # Â¶ÇÊûúË∑ùÁ¶ªË∂ÖËøáÈòàÂÄºÔºåËøîÂõûÊÉ©ÁΩö
    return torch.where(dz > z_threshold, torch.ones_like(dz), torch.zeros_like(dz))


###
#  ÂèÇËÄÉLEAP_Hand_Isaac_LabÂ•ñÂä±È°π
###

class ContinuousRotationSparseReward(ManagerTermBase):
    r"""ËøûÁª≠ÊóãËΩ¨ÁõÆÊ†áËææÊàêÁöÑÁ®ÄÁñèÂ•ñÂä±ÔºåÂÖºÂÆπ ManagerBasedRLEnv ÁöÑÊ†áÂáÜÈáçÁΩÆÊµÅÁ®ã„ÄÇ

    ËØ•Â•ñÂä±ÂáΩÊï∞‰ºöÂú®Áâ©‰ΩìÁªïÊåáÂÆöËΩ¥ÊóãËΩ¨Ë∂ÖËøá‰∏ÄÂÆöËßíÂ∫¶Êó∂Áªô‰∫àÂ•ñÂä±ÔºåÂπ∂ËÆ∞ÂΩïÊóãËΩ¨Ê¨°Êï∞„ÄÇ
    Â•ñÂä±ÂÄº‰ºöÈöèÁùÄËøûÁª≠ÊàêÂäüÊóãËΩ¨ÁöÑÊ¨°Êï∞Â¢ûÂä†ËÄåÂ¢ûÂä†„ÄÇ

    Args:
        env: ManagerBasedRLEnv ÁéØÂ¢ÉÂÆû‰æã
        asset_cfg (SceneEntityCfg, optional): Áâ©‰ΩìËµÑ‰∫ßÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫ SceneEntityCfg("object")
        theta_goal (float, optional): ÁõÆÊ†áÊóãËΩ¨ËßíÂ∫¶ÔºàÂºßÂ∫¶ÔºâÔºåÈªòËÆ§‰∏∫ œÄ/3 (Á∫¶60Â∫¶)
        additive_reward (float, optional): ÊØèÊ¨°ÊàêÂäüÊóãËΩ¨ÂêéÁ¥ØÂä†ÁöÑÂ•ñÂä±ÂÄºÔºåÈªòËÆ§‰∏∫ 0.0
        
    Returns:
        Â•ñÂä±ÂÄº (num_envs,)

    Note
    ----
        Â•ñÂä±ÂÖ¨ÂºèÔºör = 1.0 + additive_reward * count
            - count: ËøûÁª≠ÊàêÂäüÊóãËΩ¨ÁöÑÊ¨°Êï∞ÔºåÈáçÁΩÆÊó∂Ê∏ÖÈõ∂
            - additive_reward: Â•ñÂä±ÁöÑÊùÉÈáç
            - 1.0: Âü∫ÂáÜÂ•ñÂä±ÂÄº
    """
    def __init__(self, cfg, env: ManagerBasedRLEnv):
        super().__init__(cfg, env)

        self.asset_cfg: SceneEntityCfg = cfg.params.get("asset_cfg", SceneEntityCfg("object"))
        self._asset: RigidObject = env.scene[self.asset_cfg.name]

        self._theta_goal_default = float(cfg.params.get("theta_goal", 1.0471975512))
        self._additive_reward_default = float(cfg.params.get("additive_reward", 0.0))

        self._norm_eps = 1e-9
        self._angle_eps = 1e-6

        self._reset_internal_buffers()

    def reset(self, env_ids: Sequence[int] | None = None) -> None:
        q_curr, default_quat = self._read_orientations()
        self._ensure_buffers(q_curr, default_quat)

        env_ids_tensor = self._normalize_env_ids(env_ids)
        reference = self._resolve_reset_reference(default_quat, q_curr)

        self._q_last[env_ids_tensor] = reference[env_ids_tensor]
        self._next_reward[env_ids_tensor] = 1.0
        self._count[env_ids_tensor] = 0
        if self._default_quat_cache is not None:
            self._default_quat_cache[env_ids_tensor] = default_quat[env_ids_tensor]

    def __call__(
        self,
        env: ManagerBasedRLEnv,
        asset_cfg: SceneEntityCfg | None = None,
        theta_goal: float | None = None,
        additive_reward: float | None = None,
    ) -> torch.Tensor:
        if env is not self._env:
            raise ValueError("ContinuousRotationSparseReward Êé•Êî∂Âà∞ÁöÑ env ‰∏éÂàùÂßãÂåñÊó∂‰∏ç‰∏ÄËá¥")

        asset = self._asset if asset_cfg is None else env.scene[asset_cfg.name]
        q_curr = asset.data.root_quat_w.to(device=self.device)
        default_quat = asset.data.default_root_state[:, 3:7].to(device=self.device, dtype=q_curr.dtype)

        self._ensure_buffers(q_curr, default_quat)
        if self._default_quat_cache is not None:
            self._default_quat_cache.copy_(default_quat)

        theta_threshold, additive = self._normalize_parameters(theta_goal, additive_reward)
        target_axis = self._fetch_target_axis(q_curr.dtype)

        theta_signed, theta_abs = self._compute_signed_angle(q_curr, target_axis)
        self._theta_signed = theta_signed
        self._theta_abs = theta_abs

        reward = self._apply_success(theta_signed >= theta_threshold, q_curr, additive)
        return reward

    # ------------------------------------------------------------------
    # helpers
    # ------------------------------------------------------------------

    def _reset_internal_buffers(self) -> None:
        self._q_last: torch.Tensor | None = None
        self._next_reward: torch.Tensor | None = None
        self._count: torch.Tensor | None = None
        self._theta_signed: torch.Tensor | None = None
        self._theta_abs: torch.Tensor | None = None
        self._default_quat_cache: torch.Tensor | None = None

    def _read_orientations(self) -> tuple[torch.Tensor, torch.Tensor]:
        q_curr = self._asset.data.root_quat_w.to(device=self.device)
        default_quat = self._asset.data.default_root_state[:, 3:7].to(device=self.device, dtype=q_curr.dtype)
        return q_curr, default_quat

    def _ensure_buffers(self, q_curr: torch.Tensor, default_quat: torch.Tensor) -> None:
        if (
            self._q_last is None
            or self._q_last.shape[0] != self.num_envs
            or self._q_last.device != q_curr.device
            or self._q_last.dtype != q_curr.dtype
        ):
            self._initialize_buffers(q_curr, default_quat)

    def _initialize_buffers(self, q_curr: torch.Tensor, default_quat: torch.Tensor) -> None:
        reference = self._resolve_reset_reference(default_quat, q_curr)

        self._q_last = reference.clone()
        self._next_reward = torch.ones(self.num_envs, dtype=torch.float32, device=self.device)
        self._count = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
        self._theta_signed = torch.zeros(self.num_envs, dtype=q_curr.dtype, device=self.device)
        self._theta_abs = torch.zeros(self.num_envs, dtype=q_curr.dtype, device=self.device)
        self._default_quat_cache = default_quat.clone()

    def _normalize_env_ids(self, env_ids: Sequence[int] | torch.Tensor | None) -> torch.Tensor:
        if env_ids is None:
            return torch.arange(self.num_envs, device=self.device, dtype=torch.long)
        if isinstance(env_ids, torch.Tensor):
            return env_ids.to(device=self.device, dtype=torch.long)
        return torch.as_tensor(env_ids, device=self.device, dtype=torch.long)

    def _resolve_reset_reference(self, default_quat: torch.Tensor, q_curr: torch.Tensor) -> torch.Tensor:
        default_norm = default_quat / torch.linalg.norm(default_quat, dim=-1, keepdim=True).clamp_min(self._norm_eps)
        current_norm = q_curr / torch.linalg.norm(q_curr, dim=-1, keepdim=True).clamp_min(self._norm_eps)

        alignment = torch.sum(default_norm * current_norm, dim=-1, keepdim=True).abs()
        same_pose = alignment >= (1.0 - 1e-4)
        return torch.where(same_pose, default_norm, current_norm)

    def _normalize_parameters(self, theta_goal: float | None, additive_reward: float | None) -> tuple[float, float]:
        resolved_theta = self._theta_goal_default if theta_goal is None else float(theta_goal)
        resolved_additive = self._additive_reward_default if additive_reward is None else float(additive_reward)
        return resolved_theta, resolved_additive

    def _fetch_target_axis(self, dtype: torch.dtype) -> torch.Tensor:
        if not hasattr(self._env, "command_manager"):
            raise AttributeError("ContinuousRotationSparseReward ÈúÄË¶Å command_manager Êèê‰æõ rotation_axis")

        rotation_axis = self._env.command_manager.get_command("rotation_axis")
        if rotation_axis is None:
            raise ValueError("command_manager.get_command('rotation_axis') ËøîÂõû None")

        rotation_axis = rotation_axis.to(device=self.device, dtype=dtype)
        axis_norm = torch.linalg.norm(rotation_axis, dim=-1, keepdim=True).clamp_min(self._angle_eps)
        return rotation_axis / axis_norm

    def _compute_signed_angle(
        self,
        q_curr: torch.Tensor,
        target_axis: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        quat_diff = quat_mul(q_curr, quat_conjugate(self._q_last))  # type: ignore[arg-type]
        quat_diff = quat_diff / torch.linalg.norm(quat_diff, dim=-1, keepdim=True).clamp_min(self._norm_eps)
        quat_diff = torch.where((quat_diff[:, 0] < 0.0).unsqueeze(-1), -quat_diff, quat_diff)

        vector_part = quat_diff[:, 1:]
        vector_norm = torch.linalg.norm(vector_part, dim=-1, keepdim=True)
        w = torch.clamp(quat_diff[:, 0], -1.0, 1.0)
        theta = 2.0 * torch.atan2(vector_norm.squeeze(-1), w)

        axis = torch.where(vector_norm > self._angle_eps, vector_part / vector_norm, target_axis)
        rot_vec = axis * theta.unsqueeze(-1)
        theta_signed = torch.sum(rot_vec * target_axis, dim=-1)

        return theta_signed, torch.abs(theta_signed)

    def _apply_success(
        self,
        success: torch.Tensor,
        q_curr: torch.Tensor,
        additive_reward: float,
    ) -> torch.Tensor:
        reward = torch.zeros(self.num_envs, dtype=torch.float32, device=self.device)
        if torch.any(success):
            reward[success] = self._next_reward[success]
            self._q_last[success] = q_curr[success]
            self._count[success] += 1
            self._next_reward[success] = self._next_reward[success] + additive_reward
        return reward


